{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esperimento 1 - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Esperimento:** riconoscere simmetrie su un array tramite l'uso di un'architettura neurale, composta da un hidden layer con 2 neuroni e un output layer con un neurone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inizializzazione parametri\n",
    "Iniziamo importando le librerie necessarie e definendo i parametri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.1\n",
    "momentum = 0.95\n",
    "array_size = 6    # E' possibile selezionare arrays di dimensione più grande, nel qual caso l'architettura si adatterà automaticamente.\n",
    "gamma = 1.0       # Nel caso si voglia impostare un learning rate che diminuisce progressivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni\n",
    "Definiamo una funzione che restituisce 1 se l'array è simmetrico, 0 altrimenti. Essa verrà eseguita sul dataset per ottenere i labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSymmetric(a):\n",
    "    for idx in range(int(len(a)/2)):\n",
    "        if a[idx]!=a[len(a)-idx-1]:\n",
    "            return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo una funzione capace di generare tutte le 64 (2^n) combinazioni di array di lunghezza 6 (n) composte da 0 e 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_product(*arrays):\n",
    "    grid = np.meshgrid(*arrays)        \n",
    "    coord_list = [entry.ravel() for entry in grid]\n",
    "    points = np.vstack(coord_list).T\n",
    "    return points\n",
    "\n",
    "perms = cartesian_product(*array_size*[np.arange(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inizializzazione dataset e architettura\n",
    "Andiamo a definire il nostro dataset: esso è composto da array di valore 0,1 di lunghezza 6 (n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 0.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 1., 1.],\n",
      "        [0., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 1.],\n",
      "        [0., 0., 1., 1., 1., 0.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1., 1.],\n",
      "        [1., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 1., 1., 0.],\n",
      "        [1., 0., 0., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1.],\n",
      "        [1., 0., 1., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 0., 0.],\n",
      "        [1., 0., 1., 1., 0., 1.],\n",
      "        [1., 0., 1., 1., 1., 0.],\n",
      "        [1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 1., 1.],\n",
      "        [0., 1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 1.],\n",
      "        [0., 1., 0., 1., 1., 0.],\n",
      "        [0., 1., 0., 1., 1., 1.],\n",
      "        [0., 1., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 0., 0., 1.],\n",
      "        [0., 1., 1., 0., 1., 0.],\n",
      "        [0., 1., 1., 0., 1., 1.],\n",
      "        [0., 1., 1., 1., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 1.],\n",
      "        [0., 1., 1., 1., 1., 0.],\n",
      "        [0., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 1.],\n",
      "        [1., 1., 0., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 0., 0.],\n",
      "        [1., 1., 0., 1., 0., 1.],\n",
      "        [1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 1.],\n",
      "        [1., 1., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 1.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "torch.manual_seed(0)\n",
    "\n",
    "X = torch.tensor(np.array(np.meshgrid(perms)).T.reshape(-1, array_size)).float()\n",
    "Y = torch.tensor(np.apply_along_axis(isSymmetric, 1, X)).float()    # Applichiamo la funzione isSymmetric al dataset per ottenere le labels\n",
    "\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X,Y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo il modello da utilizzare, inizializzando i pesi tra -0.3 e 0.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0498,  0.1322, -0.2999, -0.1186, -0.2119, -0.2446],\n",
      "        [-0.1882, -0.0927, -0.0619,  0.0233, -0.0485,  0.1111]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1773,  0.2269]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, inputs=6):\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.Linear(in_features=inputs, out_features=2, bias=True)\n",
    "        self.out = torch.nn.Linear(in_features=2, out_features=1, bias=True)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "        # Inizializzazione pesi\n",
    "        self.layer.weight.data=torch.tensor((np.random.uniform(low = -0.3, high = 0.3, size=(2,inputs)))).float()\n",
    "        self.out.weight.data=torch.tensor((np.random.uniform(low = -0.3, high = 0.3, size=(1,2)))).float()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.activation(self.layer(x))\n",
    "        x=self.activation(self.out(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model(inputs=array_size)\n",
    "\n",
    "print(model.layer.weight)\n",
    "print(model.out.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo l'optimizer, lo scheduler e la funzione loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allenamento architettura\n",
    "Alleniamo quindi l'architettura sul dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Loss:  0.267031654715538\n",
      "Epoch:  200\n",
      "Loss:  0.10921449877787381\n",
      "Epoch:  400\n",
      "Loss:  0.09309924021363258\n",
      "Epoch:  600\n",
      "Loss:  0.01585260080082662\n",
      "Epoch:  800\n",
      "Loss:  0.0028608086504391395\n",
      "Epoch:  1000\n",
      "Loss:  0.0012804762382074841\n",
      "Epoch:  1200\n",
      "Loss:  0.0007857828113628784\n",
      "Epoch:  1400\n",
      "Loss:  0.0005556969728601757\n",
      "Epoch:  1600\n",
      "Loss:  0.0004234138199166182\n",
      "Epoch:  1800\n",
      "Loss:  0.0003399314450440727\n",
      "[0.26703165 0.13060955 0.10662537 ... 0.00028323 0.00028331 0.00028341]\n",
      "Parameter containing:\n",
      "tensor([[ 11.5517,   5.7982,  -2.9075,   2.9054,  -5.8000, -11.5517],\n",
      "        [-11.5520,  -5.7996,   2.9057,  -2.9077,   5.7985,  11.5517]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-14.6746, -14.6748]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss=0\n",
    "    model.train()\n",
    "    if int(epoch%(epochs/10)) == 0:\n",
    "        print(\"Epoch: \", epoch)\n",
    "    for x, y in iter(dataloader):\n",
    "        out=model(x)\n",
    "        l=loss(out, y.unsqueeze(1))\n",
    "        epoch_loss+=l.item()\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(epoch_loss/len(dataloader))\n",
    "    if int(epoch%(epochs/10)) == 0:\n",
    "        print(\"Loss: \", losses[epoch])\n",
    "    scheduler.step()\n",
    "\n",
    "print(np.array(losses))\n",
    "print(model.layer.weight)\n",
    "print(model.layer.bias)\n",
    "print(model.out.weight)\n",
    "print(model.out.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Osservazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I pesi ottenuti dall'architettura possiedono una simmetria analoga a quella presentata nell'articolo (i pesi simmetrici rispetto alla metà tendono ad avere modulo uguale e segno opposto, con i pesi del neurone di output negativi).<br><br>\n",
    "E' stato anche osservato che la quantità di epoche richieste per raggiungere questa particolare simmetria varia molto in base all'inizializzazione dei pesi dell'architettura (è stato necessario forzare il seed per ottenere un risultato consistente), e che la funzione loss tende a ristagnare su un valore di ~0.1 per gli array di dimensione 6, probabilmente a causa di un minimo locale. Questo fenomeno è osservabile su array di ogni dimensione, a valori di loss diversi. La causa è molto probabilmente dovuta al fatto che la maggior parte degli array di lunghezza 6 non sono simmetrici, portando così la funzione verso un'approssimazione sbagliata a 0.<br><br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e611b0851ec25071b4b85132416ea565c11a628bf418623b3546e687a0b7c369"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
